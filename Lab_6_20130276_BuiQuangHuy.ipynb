{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buiquanghuy20130276/Github-Accounts_ML2023/blob/master/Lab_6_20130276_BuiQuangHuy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This lab is to deal with classification task using **Random Forests** and **Na√Øve Bayes** algorithms with/without **Feature Selection**. \n",
        "\n",
        "*   **Deadline: 23:59, 25/03/2023**\n",
        "\n"
      ],
      "metadata": {
        "id": "LMzehe0sy5wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "H4nJmxp9zGX4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DoVWQ8AEyc-C"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "from sklearn import svm\n",
        "from sklearn import datasets\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from prettytable import PrettyTable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1. \n",
        "Task 1. Compare the performance of selected classification algorithms including **Random forest**, **NaiveBayes**, and **SVM** with **mnist** dataset based on **accuracy, precision, recall, f1** measures according to **without using selection feature** and **using selection feature**.\n",
        "\n"
      ],
      "metadata": {
        "id": "kNv07ARGzOUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "# code\n",
        "from sklearn import datasets\n",
        "mnist = datasets.load_digits()\n",
        "#RandomForest\n",
        "\n",
        "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(mnist['data'], mnist['target'], test_size=0.3, random_state=1)\n",
        "classifier.fit(X_train,y_train)\n",
        "y_preRD = classifier.predict(X_test)\n",
        "\n",
        "clf = svm.SVC(kernel='linear') \n",
        "clf.fit(X_train,y_train)\n",
        "y_predSvm = clf.predict(X_test)\n",
        "\n",
        "clf2 = GaussianNB()\n",
        "clf2.fit(X_train,y_train)\n",
        "y_predGau = clf2.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "clf4 = BernoulliNB()\n",
        "clf4.fit(X_train,y_train)\n",
        "y_predBer = clf4.predict(X_test)\n",
        "\n",
        "clf5 =  MultinomialNB()\n",
        "clf5.fit(X_train,y_train)\n",
        "y_predMul = clf5.predict(X_test)\n",
        "\n",
        "clf5 =  ComplementNB()\n",
        "clf5.fit(X_train,y_train)\n",
        "y_predComp = clf5.predict(X_test)\n",
        "\n",
        "t = PrettyTable([\"Classifier(non select Feature)\",\"accuracy\",\"precision_score\",\"recall\",\"f1\"])\n",
        "t.add_row([\"Random forest\",accuracy_score(y_preRD,y_predSvm),\n",
        "           precision_score(y_preRD,y_predSvm,average='macro'),\n",
        "           recall_score(y_preRD,y_predSvm,average='macro'),\n",
        "           f1_score(y_preRD,y_predSvm,average='macro')])\n",
        "t.add_row([\" NaiveBayes(Gauss)\",accuracy_score(y_predGau,y_predSvm),\n",
        "           precision_score(y_predGau,y_predSvm,average='macro'),\n",
        "           recall_score(y_predGau,y_predSvm,average='macro'),\n",
        "           f1_score(y_predGau,y_predSvm,average='macro')])\n",
        "\n",
        "t.add_row([\" NaiveBayes(Bernoulli)\",accuracy_score(y_predBer,y_predSvm),\n",
        "           precision_score(y_predBer,y_predSvm,average='macro'),\n",
        "           recall_score(y_predBer,y_predSvm,average='macro'),\n",
        "           f1_score(y_predBer,y_predSvm,average='macro')])\n",
        "t.add_row([\" NaiveBayes(Multinomial)\",accuracy_score(y_predMul,y_predSvm),\n",
        "           precision_score(y_predMul,y_predSvm,average='macro'),\n",
        "           recall_score(y_predMul,y_predSvm,average='macro'),\n",
        "           f1_score(y_predMul,y_predSvm,average='macro')])\n",
        "t.add_row([\" NaiveBayes(Complement)\",accuracy_score(y_predComp,y_predSvm),\n",
        "           precision_score(y_predComp,y_predSvm,average='macro'),\n",
        "           recall_score(y_predComp,y_predSvm,average='macro'),\n",
        "           f1_score(y_predComp,y_predSvm,average='macro')])\n",
        "print(t)\n",
        "X=mnist.data\n",
        "y= mnist.target\n",
        "X_Select = SelectKBest(chi2,k=10).fit_transform(X,y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_Select, y, test_size=0.3, random_state=1)\n",
        "\n",
        "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "classifier.fit(X_train,y_train)\n",
        "y_preRD = classifier.predict(X_test)\n",
        "\n",
        "clf = svm.SVC(kernel='linear') \n",
        "clf.fit(X_train,y_train)\n",
        "y_predSvm = clf.predict(X_test)\n",
        "\n",
        "clf2 = GaussianNB()\n",
        "clf2.fit(X_train,y_train)\n",
        "y_predGau = clf2.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "clf4 = BernoulliNB()\n",
        "clf4.fit(X_train,y_train)\n",
        "y_predBer = clf4.predict(X_test)\n",
        "\n",
        "clf5 =  MultinomialNB()\n",
        "clf5.fit(X_train,y_train)\n",
        "y_predMul = clf5.predict(X_test)\n",
        "\n",
        "clf5 =  ComplementNB()\n",
        "clf5.fit(X_train,y_train)\n",
        "y_predComp = clf5.predict(X_test)\n",
        "\n",
        "t = PrettyTable([\"Classifier(select Feature)\",\"accuracy\",\"precision_score\",\"recall\",\"f1\"])\n",
        "t.add_row([\"Random forest\",accuracy_score(y_preRD,y_predSvm),\n",
        "           precision_score(y_preRD,y_predSvm,average='macro'),\n",
        "           recall_score(y_preRD,y_predSvm,average='macro'),\n",
        "           f1_score(y_preRD,y_predSvm,average='macro')])\n",
        "t.add_row([\" NaiveBayes(Gauss)\",accuracy_score(y_predGau,y_predSvm),\n",
        "           precision_score(y_predGau,y_predSvm,average='macro'),\n",
        "           recall_score(y_predGau,y_predSvm,average='macro'),\n",
        "           f1_score(y_predGau,y_predSvm,average='macro')])\n",
        "\n",
        "t.add_row([\" NaiveBayes(Bernoulli)\",accuracy_score(y_predBer,y_predSvm),\n",
        "           precision_score(y_predBer,y_predSvm,average='macro'),\n",
        "           recall_score(y_predBer,y_predSvm,average='macro'),\n",
        "           f1_score(y_predBer,y_predSvm,average='macro')])\n",
        "t.add_row([\" NaiveBayes(Multinomial)\",accuracy_score(y_predMul,y_predSvm),\n",
        "           precision_score(y_predMul,y_predSvm,average='macro'),\n",
        "           recall_score(y_predMul,y_predSvm,average='macro'),\n",
        "           f1_score(y_predMul,y_predSvm,average='macro')])\n",
        "t.add_row([\" NaiveBayes(Complement)\",accuracy_score(y_predComp,y_predSvm),\n",
        "           precision_score(y_predComp,y_predSvm,average='macro'),\n",
        "           recall_score(y_predComp,y_predSvm,average='macro'),\n",
        "           f1_score(y_predComp,y_predSvm,average='macro')])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "sOsg77IBzEyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c330386f-61ab-456b-c055-d12769fe5138"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "| Classifier(non select Feature) |      accuracy      |  precision_score   |       recall       |         f1         |\n",
            "+--------------------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|         Random forest          | 0.8574074074074074 | 0.8550721355315943 | 0.8636044175912663 | 0.850581821905301  |\n",
            "|        NaiveBayes(Gauss)       | 0.8296296296296296 | 0.8286636821931188 | 0.8534973294935201 | 0.8267507799833538 |\n",
            "|      NaiveBayes(Bernoulli)     | 0.8592592592592593 | 0.8561654526607058 | 0.855629463857291  | 0.8531450317995708 |\n",
            "|     NaiveBayes(Multinomial)    | 0.8925925925925926 | 0.8914825989230681 | 0.8944378399576791 | 0.888560757214157  |\n",
            "|     NaiveBayes(Complement)     | 0.8203703703703704 | 0.8130615046232794 | 0.8470504227656275 | 0.8061295298360603 |\n",
            "+--------------------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "+----------------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "| Classifier(select Feature) |      accuracy      |  precision_score   |       recall       |         f1         |\n",
            "+----------------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       Random forest        | 0.7351851851851852 | 0.7329634514640702 | 0.7297938552470169 | 0.7047378518933707 |\n",
            "|      NaiveBayes(Gauss)     | 0.6203703703703703 | 0.6133712733146328 | 0.790093591838678  | 0.6318463010417356 |\n",
            "|    NaiveBayes(Bernoulli)   | 0.7296296296296296 | 0.7236406810059128 | 0.7219780548526193 | 0.7182356990121661 |\n",
            "|   NaiveBayes(Multinomial)  | 0.8555555555555555 | 0.8578783788424694 | 0.8557714431861291 | 0.8493928092690931 |\n",
            "|   NaiveBayes(Complement)   | 0.7685185185185185 | 0.7575654505270829 | 0.8001182006961315 | 0.7561674158183961 |\n",
            "+----------------------------+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2. \n",
        "For given bank dataset (bank.csv) having the following attributes :\n",
        "1.\t**age** (numeric)\n",
        "2.\t**job** : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
        "3.\t**marital** : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
        "4.\t**education** (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
        "5.\t**default**: has credit in default? (categorical: 'no','yes','unknown')\n",
        "6.\t**housing**: has housing loan? (categorical: 'no','yes','unknown')\n",
        "7.\t**loan**: has personal loan? (categorical: 'no','yes','unknown')\n",
        "8.\t**contact**: contact communication type (categorical: 'cellular','telephone')\n",
        "9.\t**month**: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
        "10.\t**day_of_week**: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
        "11.\t**duration**: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
        "12.\t**campaign**: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
        "13.\t**pdays**: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
        "14.\t**previous**: number of contacts performed before this campaign and for this client (numeric)\n",
        "15.\t**poutcome**: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
        "Output variable (desired target):\n",
        "16.\t**y**. has the client subscribed a term deposit? (binary: 'yes','no')\n",
        "\n"
      ],
      "metadata": {
        "id": "b52OPWPD2afi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd '/content/gdrive/MyDrive/lab6'\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n"
      ],
      "metadata": {
        "id": "N7yqC5fRpQF_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17eda0d1-fcd0-4eb9-b546-61c924827d6b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/MyDrive/lab6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1. Apply StandardScaler() function to columns that contains numerical data ('age', 'balance', 'day', 'campaign', 'pdays', 'previous')"
      ],
      "metadata": {
        "id": "q89LEvT7dqaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code\n",
        "data = pd.read_csv('bank.csv')\n",
        "colunm =['age','balance','day','campaign','pdays','previous']\n",
        "scaler = StandardScaler();\n",
        "scaler.fit(data[colunm])\n",
        "scaler_data = scaler.transform(data[colunm])\n",
        "data[colunm]=scaler_data\n",
        "print(data[colunm])\n",
        "\n"
      ],
      "metadata": {
        "id": "8vx3mfIidu4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16263b44-3f87-4c41-a378-97a94ae5bb14"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            age   balance       day  campaign     pdays  previous\n",
            "0      1.491505  0.252525 -1.265746 -0.554168 -0.481184 -0.363260\n",
            "1      1.239676 -0.459974 -1.265746 -0.554168 -0.481184 -0.363260\n",
            "2     -0.019470 -0.080160 -1.265746 -0.554168 -0.481184 -0.363260\n",
            "3      1.155733  0.293762 -1.265746 -0.554168 -0.481184 -0.363260\n",
            "4      1.071790 -0.416876 -1.265746 -0.186785 -0.481184 -0.363260\n",
            "...         ...       ...       ...       ...       ...       ...\n",
            "11157 -0.691015 -0.473616  0.515650 -0.554168 -0.481184 -0.363260\n",
            "11158 -0.187357 -0.246658  0.040612  0.547981 -0.481184 -0.363260\n",
            "11159 -0.774958 -0.464934  0.396891 -0.186785 -0.481184 -0.363260\n",
            "11160  0.148416 -0.473926 -0.909466 -0.186785  1.109571  1.818332\n",
            "11161 -0.607072 -0.473926 -0.790707 -0.554168 -0.481184 -0.363260\n",
            "\n",
            "[11162 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.2. Apply Encode Categorical Value (OneHotEncoder) to transfrom categorical data to numerical data ('job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome')"
      ],
      "metadata": {
        "id": "r7acR0TxdvY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code\n",
        "\n",
        "colunm1 =['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "encode = OneHotEncoder();\n",
        "encode.fit(data[colunm1])\n",
        "encode_data = encode.transform(data[colunm1])\n",
        "encode_df = pd.DataFrame(encode_data.toarray())\n",
        "data.drop(colunm1,axis=1,inplace=True)\n",
        "data =pd.concat([data,encode_df],axis=1)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "egtgBmAtd0um",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20762b2a-cabd-4070-e2c9-8c98668c8247"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       age  balance  day  duration  campaign  pdays  previous deposit    0  \\\n",
            "0       59     2343    5      1042         1     -1         0     yes  1.0   \n",
            "1       56       45    5      1467         1     -1         0     yes  1.0   \n",
            "2       41     1270    5      1389         1     -1         0     yes  0.0   \n",
            "3       55     2476    5       579         1     -1         0     yes  0.0   \n",
            "4       54      184    5       673         2     -1         0     yes  1.0   \n",
            "...    ...      ...  ...       ...       ...    ...       ...     ...  ...   \n",
            "11157   33        1   20       257         1     -1         0      no  0.0   \n",
            "11158   39      733   16        83         4     -1         0      no  0.0   \n",
            "11159   32       29   19       156         2     -1         0      no  0.0   \n",
            "11160   43        0    8         9         2    172         5      no  0.0   \n",
            "11161   34        0    9       628         1     -1         0      no  0.0   \n",
            "\n",
            "         1  ...   34   35   36   37   38   39   40   41   42   43  \n",
            "0      0.0  ...  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
            "1      0.0  ...  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
            "2      0.0  ...  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
            "3      0.0  ...  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
            "4      0.0  ...  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
            "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
            "11157  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
            "11158  0.0  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
            "11159  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
            "11160  0.0  ...  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
            "11161  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
            "\n",
            "[11162 rows x 52 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.3. Apply **Decision tree, Random forest, kNN, Na√ØveBayes** to preproceed dataset in the previous steps. Then compare the obtained results using **accuracy, precision, recall, f1** measures."
      ],
      "metadata": {
        "id": "K2Si6d69d1nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(data.drop('deposit', axis=1), data['deposit'], test_size=0.3, random_state=42)\n",
        "# #Decision Tree\n",
        "# tree = DecisionTreeClassifier(criterion=\"gini\", random_state=42, max_depth=5, min_samples_leaf=10)\n",
        "# tree.fit(X_train, y_train)\n",
        "# y_pred_tree = tree.predict(X_test)\n",
        "# #Random forest\n",
        "# clf_randomforest=RandomForestClassifier(n_estimators=100)\n",
        "# clf_randomforest.fit(X_train,y_train)\n",
        "# y_pred_random = clf_randomforest.predict(X_test)\n",
        "# #kNN\n",
        "# knn = KNeighborsClassifier(n_neighbors=5)\n",
        "# knn.fit(X_train, y_train)\n",
        "# y_pred_knn = knn.predict(X_test)\n",
        "# #NaiveBayes\n",
        "# model = GaussianNB()\n",
        "# model.fit(X_train, y_train)\n",
        "# y_pred_bayes = model.predict(X_test)\n",
        "# table = {\n",
        "#     \"Decision Tree\":{\n",
        "#         \"accuracy\": accuracy_score(y_test, y_pred_tree),\n",
        "#         \"precision\": precision_score(y_test, y_pred_tree, average='macro',zero_division=1),\n",
        "#         \"recall\": recall_score(y_test, y_pred_tree, average='macro'),\n",
        "#         \"f1\": f1_score(y_test, y_pred_tree, average='macro')\n",
        "#     },\n",
        "#     \"Random forest\":{\n",
        "#         \"accuracy\": accuracy_score(y_test, y_pred_random),\n",
        "#         \"precision\": precision_score(y_test, y_pred_random, average='macro',zero_division=1),\n",
        "#         \"recall\": recall_score(y_test, y_pred_random, average='macro'),\n",
        "#         \"f1\": f1_score(y_test, y_pred_random, average='macro')\n",
        "#     },\n",
        "#     \"kNN\":{\n",
        "#         \"accuracy\": accuracy_score(y_test, y_pred_knn),\n",
        "#         \"precision\": precision_score(y_test, y_pred_knn, average='macro',zero_division=1),\n",
        "#         \"recall\": recall_score(y_test, y_pred_knn, average='macro'),\n",
        "#         \"f1\": f1_score(y_test, y_pred_knn, average='macro')\n",
        "#     },\n",
        "#     \"NaiveBayes\":{\n",
        "#         \"accuracy\": accuracy_score(y_test, y_pred_bayes),\n",
        "#         \"precision\": precision_score(y_test, y_pred_bayes, average='macro',zero_division=1),\n",
        "#         \"recall\": recall_score(y_test, y_pred_bayes, average='macro'),\n",
        "#         \"f1\": f1_score(y_test, y_pred_bayes, average='macro')\n",
        "#     },  \n",
        "# }\n",
        "# #print\n",
        "# df_metrics = pd.DataFrame.from_dict(table, orient='index')\n",
        "# print(\"Without using selection feature\")\n",
        "# print(df_metrics)"
      ],
      "metadata": {
        "id": "Ouil-cf_d8jW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "231e416f-1adc-4f03-dc65-023808933797"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-c7e1345d771d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Decision Tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gini\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_pred_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Random forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcheck_X_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfeature_names_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfeature_names_in\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names_in_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_names_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_get_feature_names\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m   1901\u001b[0m     \u001b[0;31m# mixed type of string and non-string is not supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"str\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1903\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m   1904\u001b[0m             \u001b[0;34m\"Feature names are only supported if all input features have string names, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m             \u001b[0;34mf\"but your input has {types} as feature name / column name types. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Feature names are only supported if all input features have string names, but your input has ['int', 'str'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.4. Using selection feature to above dataset, then compare the classification results with those in Task 2.3. "
      ],
      "metadata": {
        "id": "SweVRB4meApP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "seFBhqCSeC7C"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 4. \n",
        "For a given dataset in the Lab #5 (**credit card dataset**), perform feature selection and thencompare the performance of selected classification algorithms (Decision Tree, kNN, Logistic Regression, SVM, Random Forest and NaiveBayes) based on accuracy, precision, recall, f1 measures.\n"
      ],
      "metadata": {
        "id": "Z5pp7_h-aP2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code"
      ],
      "metadata": {
        "id": "Rw_-8FIf2KxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finally,\n",
        "Save a copy in your Github. Remember renaming the notebook."
      ],
      "metadata": {
        "id": "Ok7RGkea_b7n"
      }
    }
  ]
}